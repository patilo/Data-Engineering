#aqui les dejo una guia de como realizar practicas 
#en la ingenieria de datos
#las herramientas que utilizaremos son: s3 - rds - python - sql
#crearemos datos ficticios y luego los cargaremos en aws

## generamos dataset ficticio con Python
import pandas as pd
import random
from faker import Faker

#configuramos generador de datos ficticios
faker = Faker()
categorias = ['Electrónica', 'Ropa', 'Hogar', 'Alimentos']
sucursales = ['Santiago', 'Valparaíso', 'Concepción', 'Antofagasta']
metodos_pago = ['Débito', 'Crédito', 'Transferencia']
clientes = ['Cliente', 'No Cliente']

#creamos datos simulados
data = []
for _ in range(1000):  # Generar 1000 registros
    data.append({
        'producto': faker.word(),
        'precio': round(random.uniform(10, 1000), 2),
        'fecha': faker.date_between(start_date='-1y', end_date='today'),
        'sucursal': random.choice(sucursales),
        'cantidad': random.randint(1, 10),
        'categoria': random.choice(categorias),
        'metodo_pago': random.choice(metodos_pago),
        'tipo_cliente': random.choice(clientes)
    })

#guardamos datos en un CSV
df = pd.DataFrame(data)
df.to_csv('ventas_simuladas.csv', index=False)
print("Archivo 'ventas_simuladas.csv' generado.")
#----------------------------------------------------------------------------------------------#




##subiremos datos a AWS S3
import boto3

#configuramos S3
bucket_name = 'mi-bucket-ejemplo'
file_name = 'ventas_simuladas.csv'

#subimos archivo
s3 = boto3.client('s3')
s3.upload_file(file_name, bucket_name, file_name)
print(f"Archivo {file_name} subido a S3 en el bucket {bucket_name}.")

##descargamos y procesamos datos desde S3
#descargamos el archivo desde S3
s3.download_file(bucket_name, file_name, f"descargado_{file_name}")

#leemos y procesamos
df = pd.read_csv(f"descargado_{file_name}")

#limpieza: Llenar nulos, calcular ventas totales
df.fillna({'cantidad': 1}, inplace=True)
df['venta_total'] = df['precio'] * df['cantidad']

#filtramos ventas mayores a $1000
df_filtrado = df[df['venta_total'] > 1000]

#guardamos datos procesados
df_filtrado.to_csv('ventas_procesadas.csv', index=False)
print("Datos procesados guardados en 'ventas_procesadas.csv'.")
#----------------------------------------------------------------------------------------------#







##subiremos datos procesados a AWS RDS
from sqlalchemy import create_engine
#configuramos conexión a RDS
engine = create_engine('postgresql://user:password@rds-endpoint.amazonaws.com/mydatabase')

#cargamos datos a RDS
df_filtrado.to_sql('ventas', engine, if_exists='replace', index=False)
print("Datos subidos a RDS.")
#----------------------------------------------------------------------------------------------#






##Consultas SQL avanzadas
#filtramos ventas mayores a $1000 realizadas con tarjeta de crédito
query_ventas = """
SELECT *
FROM ventas
WHERE metodo_pago = 'Crédito' AND venta_total > 1000;
"""

#encontrar la  sucursal con más ventas en un mes
query_sucursal = """
SELECT sucursal, EXTRACT(MONTH FROM fecha) AS mes, SUM(venta_total) AS total_ventas
FROM ventas
GROUP BY sucursal, mes
ORDER BY total_ventas DESC
LIMIT 1;
"""

#creamos índice para optimizar consultas
query_index = "CREATE INDEX idx_fecha_sucursal ON ventas(fecha, sucursal);"
#----------------------------------------------------------------------------------------------#





## Preguntas de simulación
#AWS: Subir múltiples archivos a S3
import os

for file in os.listdir('carpeta_local'):
    s3.upload_file(f'carpeta_local/{file}', bucket_name, file)
print("Archivos subidos exitosamente.")

#SQL: Detectar clientes frecuentes
query_clientes_frecuentes = """
SELECT tipo_cliente, COUNT(*) AS cantidad_compras
FROM ventas
GROUP BY tipo_cliente
HAVING COUNT(*) > 5;
"""

#Python: Detectar duplicados en un dataset
duplicados = df[df.duplicated()]
print("Duplicados encontrados:\n", duplicados)

#Python: Uso de Pandas para anlálisis exploratorio de datos
print("Estadísticas descriptivas:\n", df.describe())
print("Valores nulos:\n", df.isnull().sum())
